import argparse
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from tqdm import tqdm
import matplotlib
import wandb
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import time
from wandb.integration.sb3 import WandbCallback
from stable_baselines3.common.callbacks import EvalCallback

from model import Model
from custom_reward_wrapper import TorchPreferenceRewardNormalized, VecTorchPreferenceRewardNormalized
from gt_traj_dataset import GTTrajLevelDataset


class RandomAgent(object):
    """The world's simplest agent!"""
    def __init__(self, action_space):
        self.action_space = action_space
        self.model_path = 'random_agent'

    def act(self, observation, reward, done):
        return self.action_space.sample()[None]

def train(args: dict, run):
    logdir = Path(args.log_dir)

    if logdir.exists():
        c = 'n' # input('log dir already exists. continue to train a preference model? [Y/etc]? ')
        if len(c) == 0 or 'y' in c.lower():
            import shutil
            shutil.rmtree(str(logdir))
    else:
        logdir.mkdir(parents=True)

    with open(str(logdir/'args.txt'), 'w') as f:
        f.write(str(args))

    logdir = str(logdir)
    env = gym.make(args.env_id)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize agents
    train_agents = [RandomAgent(env.action_space)] if args.random_agent else []

    models = sorted([p for p in (Path(args.learners_path) / args.env_id).glob('*') if int(p.name.strip(".zip")) <= args.max_chkpt])
    for path in models:
        agent = PPO.load(path, device='cpu')
        setattr(agent,'model_path',str(path))
        train_agents.append(agent)

    # Initialize dataset based on preference type
    if args.preference_type == 'gt':
        dataset = GTDataset(env)
    elif args.preference_type == 'gt_traj':
        dataset = GTTrajLevelDataset(env, args.log_dir)
        val_dataset = GTTrajLevelDataset(env, args.log_dir, val=True)
    else:
        raise ValueError('Invalid preference type')

    dataset.prebuilt(train_agents, args.min_length, run)
    val_dataset.prebuilt(train_agents, args.min_length, run)

    # Initialize models
    models = []
    for i in range(args.num_models):
        model = Model(
            model_num=i,
            include_action=args.include_action,
            ob_dim=env.observation_space.shape[0],
            ac_dim=env.action_space.shape[0],
            num_layers=args.num_layers,
            embedding_dims=args.embedding_dims,
            device=device
        )
        models.append(model)

    # Train each model
    for i, model in enumerate(iterable=models):
        print(f"Training model {i+1}/{args.num_models}")
        
        model.train_model(
            dataset,
            val_dataset,
            batch_size=args.batch_size,
            num_epochs=args.iter,
            l2_reg=args.l2_reg,
            noise_level=args.noise,
            debug=True,
            run=run
        )

        # Save model
        torch.save(model.state_dict(), f"{logdir}/model_{i}.pt")
        # Save model configuration separately for loading
        model_config = {
            'include_action': args.include_action,
            'ob_dim': env.observation_space.shape[0],
            'ac_dim': env.action_space.shape[0],
            'num_layers': args.num_layers,
            'embedding_dims': args.embedding_dims
        }
        torch.save(model_config, f"{logdir}/model_{i}_config.pt")

if __name__ == "__main__":
    # Required Args (target envs & learners)
    parser = argparse.ArgumentParser(description=None)
    parser.add_argument('--env_id', default='', help='Select the environment to run')
    parser.add_argument('--env_type', default='', help='mujoco or atari')
    parser.add_argument('--learners_path', default='', help='path of learning agents')
    parser.add_argument('--max_chkpt', default=64000, type=int, help='decide upto what learner stage you want to give')
    parser.add_argument('--steps', default=40, type=int, help='length of snippets')
    parser.add_argument('--max_steps', default=100_000, type=int, help='length of max snippets (gt_traj_no_steps only)')  # idk
    parser.add_argument('--traj_noise', default=None, type=int, help='number of adjacent swaps (gt_traj_no_steps_noise only)')
    parser.add_argument('--min_length', default=1000,type=int, help='minimum length of trajectory generated by each agent')
    parser.add_argument('--num_layers', default=3, type=int, help='number layers of the reward network')
    parser.add_argument('--embedding_dims', default=256, type=int, help='embedding dims')
    parser.add_argument('--num_models', default=5, type=int, help='number of models to ensemble')
    parser.add_argument('--l2_reg', default=0, type=float, help='l2 regularization size')
    parser.add_argument('--noise', default=0, type=float, help='noise level to add on training label')
    parser.add_argument('--D', default=1000, type=int, help='|D| in the preference paper')
    parser.add_argument('--N', default=10, type=int, help='number of trajactory mix (gt_traj_no_steps_n_mix only)')
    parser.add_argument('--log_dir', required=True)
    parser.add_argument('--preference_type', default='gt_traj', help='gt or gt_traj or time or gt_traj_no_steps, gt_traj_no_steps_n_mix; if gt then preference will be given as a GT reward, otherwise, it is given as a time index')
    parser.add_argument('--min_margin', default=1, type=int, help='when prefernce type is "time", the minimum margin that we can assure there exist a margin')
    parser.add_argument('--include_action', action='store_true', help='whether to include action for the model or not')
    parser.add_argument('--stochastic', action='store_true', help='whether want to use stochastic agent or not')
    parser.add_argument('--random_agent', action='store_true', help='whether to use default random agent')
    # Args for PPO
    parser.add_argument('--rl_runs', default=1, type=int)
    parser.add_argument('--ppo_log_path', default='ppo2')
    parser.add_argument('--custom_reward', default="preference_normalized", help='preference or preference_normalized')
    parser.add_argument('--ctrl_coeff', default=0.0, type=float)
    parser.add_argument('--alive_bonus', default=0.0, type=float)
    parser.add_argument('--gamma', default=0.99, type=float)
    parser.add_argument('--ppo_policy', default="MlpPolicy", type=str)
    parser.add_argument('--batch_size', default=64, type=int)  # from paper section 5.1.2
    parser.add_argument('--iter', default=10000, type=int)
    parser.add_argument('--num_envs', default=4, type=int)
    args = parser.parse_args()

    config = {
        "final_agent": {
            "policy_type": "MlpPolicy",
            "total_timesteps": 100_000,
            "env_id": args.env_id,
        }
    }
    run = wandb.init(
        project="trex-training",
        config=config,
        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
        monitor_gym=True,  # auto-upload the videos of agents playing the game
        save_code=True,  # optional
    )
    # Train a Preference Model
    train(args, run)

    # Train an agent
    group = str(time.time())

    wandb_callback = WandbCallback(
            model_save_path=f"models/{run.id}",
            verbose=2,
        )

    agent_config = config['final_agent']
    for i in range(args.rl_runs):
        def make_indiv_env():
            env = gym.make(agent_config["env_id"])
            env = TorchPreferenceRewardNormalized(env, args.num_models, args.log_dir, args.include_action, args.num_layers, args.embedding_dims)
            return env
        env = DummyVecEnv([make_indiv_env for _ in range(args.num_envs)])
        model = PPO(
            agent_config['policy_type'], 
            env, 
            n_steps=128//args.num_envs, 
            device="cpu", 
            tensorboard_log=f"runs/{run.id}",
            verbose=0
        )

        model.learn(
            total_timesteps=agent_config["total_timesteps"],
            callback=[
                wandb_callback,
                # Add evaluation callback to monitor performance during training
                EvalCallback(
                    eval_env=make_indiv_env(),
                    eval_freq=5000,  # Evaluate every 5000 steps
                    n_eval_episodes=10,
                    best_model_save_path=f"{args.log_dir}/{run.id}/best_model",
                    log_path=f"{args.log_dir}/{run.id}",
                    deterministic=True
                )
            ],
        )
        env.close()

        # Evaluate the trained model
        eval_env = gym.make(agent_config["env_id"])
        max_xs = []
        for k in range(30):
            print(f'Evaluation Run {k}')
            obs, _ = eval_env.reset()
            total_reward = 0
            num_steps = 0
            done = False
            terminated = False
            max_x_pos = -np.inf
            
            while not done and not terminated and num_steps < 1000:
                action, _ = model.predict(obs)
                obs, reward, done, terminated, info = eval_env.step(action)
                total_reward += reward
                num_steps += 1
                max_x_pos = max(max_x_pos, info["x_position"])
                    
            run.log({
                "eval/total_reward": total_reward,
                "eval/total_steps": num_steps,
                "eval/max_x_pos": max_x_pos,
                "eval/run": k
            })
            max_xs.append(max_x_pos)
            print(f'Run {k} - Total Reward: {total_reward}, Total Steps: {num_steps}, Max X Pos: {max_x_pos}')
        print(f'Average Max X Pos: {np.mean(max_xs)} STD Max X Pos: {np.std(max_xs)}')
        eval_env.close()

        run.finish()
